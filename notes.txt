Adam, Cross entropy loss

With basic preprocessing (just resized to 128x128): 
  VGG16: poor performance

  VGG11: slightly better, but still bad on testing data (flatlined at 16 starting from epoch 4)

With more advanced preprocessing:
  Cropped to content and stretched to 64x64
  Much better, but unstable?
    Stability increased by changing learning rate from 0.001 to 0.0001

  SGD instead of Adam: poor results, no convergence

  Trying VGG16 again: no noticable difference apart from longer computation time

  Resizing from 64x64 to 32x32: Slightly better performance, at 98%

  Anything lower than 32x32 doesn't work with the VGG model. 
  However, we can still downscale from a lower resolution. 
  Testing 16x16 only resulted in a decrease from ~98% to ~97%.

  Using MNIST digits dataset: Better performance and significantly faster at reaching high test accuracy - took only 1 epoch to hit ~97% in some cases
    Why? dataset is significantly larger - training on 10,000 images vs 400
    99.29% accuracy
    Starts overfitting at 30
    
    Results:
    0: 92.12419152259827
    6126.0 / 10000 (0.6126)
    1: 55.077666997909546
    9257.0 / 10000 (0.9257)
    2: 19.50813952088356
    9639.0 / 10000 (0.9639)
    3: 5.966270640492439
    9746.0 / 10000 (0.9746)
    4: 3.17241770029068
    9771.0 / 10000 (0.9771)
    5: 2.253926556557417
    9858.0 / 10000 (0.9858)
    6: 1.7381018921732903
    9877.0 / 10000 (0.9877)
    7: 1.3911116272211075
    9829.0 / 10000 (0.9829)
    8: 1.0716251451522112
    9885.0 / 10000 (0.9885)
    9: 0.9213910736143589
    9893.0 / 10000 (0.9893)
    10: 0.9200736172497272
    9895.0 / 10000 (0.9895)
    11: 0.736453564837575
    9904.0 / 10000 (0.9904)
    12: 0.5108709558844566
    9896.0 / 10000 (0.9896)
    13: 0.4055847651325166
    9907.0 / 10000 (0.9907)
    14: 0.40341803058981895
    9899.0 / 10000 (0.9899)
    15: 0.5085174459964037
    9905.0 / 10000 (0.9905)
    16: 0.3044069716706872
    9910.0 / 10000 (0.991)
    17: 0.24592616804875433
    9916.0 / 10000 (0.9916)
    18: 0.27963892510160804
    9869.0 / 10000 (0.9869)
    19: 0.27924210811033845
    9912.0 / 10000 (0.9912)
    20: 0.3931031967513263
    9927.0 / 10000 (0.9927)
    21: 0.12686895160004497
    9924.0 / 10000 (0.9924)
    22: 0.06605914363171905
    9926.0 / 10000 (0.9926)
    23: 0.06236228544730693
    9929.0 / 10000 (0.9929)
    24: 0.07867385871941224
    9918.0 / 10000 (0.9918)
    25: 0.06353718787431717
    9929.0 / 10000 (0.9929)
    26: 0.03206191898789257
    9926.0 / 10000 (0.9926)
    27: 0.020379534631501883
    9928.0 / 10000 (0.9928)
    28: 0.026161261848756112
    9926.0 / 10000 (0.9926)
    29: 0.028937290102476254
    9921.0 / 10000 (0.9921)
    30: 0.01738140482484596
    9918.0 / 10000 (0.9918)

  Should we randomise rotation somewhat?
  Should we make it easier to distinguish between 1 and 7 somehow?

Switching to resnet:
  Strangely, 64x64 seemed to perform better than 32x32